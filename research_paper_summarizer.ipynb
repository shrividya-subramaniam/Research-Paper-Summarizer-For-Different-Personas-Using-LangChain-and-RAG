{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a412f0",
   "metadata": {},
   "source": [
    "# Research Paper Summarizer for Different Personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625a138",
   "metadata": {},
   "source": [
    "## 1.Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "826176dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Libraries\n",
    "import os\n",
    "import re\n",
    "import anthropic\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# For Document Loading\n",
    "from langchain.document_loaders import UnstructuredFileLoader, WebBaseLoader, ArxivLoader, PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# For Text Splitting and Processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# For Embedding and Vector Store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# For LLM and Prompts\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# For setting up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e40c4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read .env file to get Anthropic API key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv('env')) # read local .env file\n",
    "anthropic_api_key=os.environ['ANTHROPIC_API_KEY']\n",
    "\n",
    "# Initialize Anthropic client\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa99e4",
   "metadata": {},
   "source": [
    "## 2. Define & Initialize LLM Models & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26cfd4",
   "metadata": {},
   "source": [
    "### Define LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53095ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model for Summarization\n",
    "llm_model = \"claude-3-7-sonnet-20250219\"\n",
    "default_llm = ChatAnthropic(model=llm_model, temperature=0.0, api_key=anthropic_api_key)\n",
    "\n",
    "# LLM Model for Evaluation\n",
    "llm_eval_model = \"claude-sonnet-4-20250514\"\n",
    "default_llm_eval = ChatAnthropic(model=llm_eval_model, temperature=0.5, api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d8189",
   "metadata": {},
   "source": [
    "### Initialize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4505a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8e945",
   "metadata": {},
   "source": [
    "## 3. Create Important Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f164dcb",
   "metadata": {},
   "source": [
    "### 1. Create SentenceWindow and SentenceWindowRetriever Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c78166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceWindow and SentenceWindowRetriever classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "class SentenceWindow:\n",
    "    \"\"\" A sentence with its surrounding context window. \"\"\"\n",
    "    def __init__(self, sentence, window_context, sentence_index, metadata):\n",
    "        self.sentence = sentence\n",
    "        self.window_context = window_context\n",
    "        self.sentence_index = sentence_index\n",
    "        self.metadata = metadata\n",
    "\n",
    "class SentenceWindowRetriever:\n",
    "    \"\"\" To retrieve sentences with their context windows. \"\"\"\n",
    "    def __init__(self, window_size: int =2):\n",
    "        self.window_size = window_size\n",
    "        self.sentence_windows = []\n",
    "        self.vector_store = None\n",
    "\n",
    "    def create_sentence_windows(self, documents):\n",
    "        \"\"\" Create sentence windows from the documents\"\"\"\n",
    "        logger.info(\"Creating sentence windows....\")\n",
    "        all_windows = []\n",
    "\n",
    "        for doc in documents:\n",
    "            # Split into sentences using NLTK\n",
    "            sentences = nltk.sent_tokenize(doc.page_content)\n",
    "\n",
    "            # Creare windows around each sentence\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                # Define window boundaries\n",
    "                start_index = max(0, i - self.window_size)\n",
    "                end_index = min(len(sentences), i + self.window_size + 1)\n",
    "\n",
    "                # Create context window\n",
    "                window_sentences = sentences[start_index:end_index]\n",
    "                window_context = \" \".join(window_sentences)\n",
    "\n",
    "                # Create metadata\n",
    "                metadata = {\n",
    "                    **doc.metadata,\n",
    "                    \"sentence_index\":i,\n",
    "                    \"total_sentences\": len(sentences),\n",
    "                    \"window_start\": start_index,\n",
    "                    \"window_end\": end_index\n",
    "                }\n",
    "\n",
    "                window = SentenceWindow(\n",
    "                    sentence=sentence,\n",
    "                    window_context=window_context,\n",
    "                    sentence_index=i,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                all_windows.append(window)\n",
    "\n",
    "        logger.info(f\"Created {len(all_windows)} sentence windows.\")\n",
    "        self.sentence_windows = all_windows\n",
    "        return all_windows\n",
    "    \n",
    "    def build_vectorstore(self, embeddings):\n",
    "        \"\"\" Build a vector store from the sentence windows. \"\"\"\n",
    "        if not self.sentence_windows:\n",
    "            raise ValueError(\"No sentence windows created. Call create_sentence_windows() first.\")\n",
    "        \n",
    "        logger.info(\"Building vector store from sentence windows...\")\n",
    "\n",
    "        # Convert windows to documents for vector store\n",
    "        window_docs = []\n",
    "        for window in self.sentence_windows:\n",
    "            doc = Document(\n",
    "                page_content=window.window_context,\n",
    "                metadata={\n",
    "                    **window.metadata,\n",
    "                    \"core_sentence\": window.sentence\n",
    "                }\n",
    "            )\n",
    "            window_docs.append(doc)\n",
    "\n",
    "        # Build FAISS vector store (more efficient than in-memory)\n",
    "        self.vectorstore = FAISS.from_documents(window_docs, embeddings)\n",
    "        logger.info(\"Vector store built successfully.\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\" Retrieve relevant sentence windows \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vector store not built. Call build_vectorstore() first.\")\n",
    "        \n",
    "        return self.vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "print(\"SentenceWindow and SentenceWindowRetriever classes defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20be74",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77339a17",
   "metadata": {},
   "source": [
    "### 2. Create DocumentProcessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43ccf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentProcessor class defined.\n"
     ]
    }
   ],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\" Handles document loading and preprocessing\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_documents(source):\n",
    "        \"\"\" Load document from  various sources with better error handling\"\"\"\n",
    "        logger.info(f\"Loading document from {source}\")\n",
    "\n",
    "        try:\n",
    "            # Determine the source type(whether it's a PDF file or a ARXIV link) and use appropriate loader\n",
    "            if source.startswith(\"http\") and \"arxiv.org\" in source:\n",
    "                # Extract arXiv ID from URL\n",
    "                arxiv_id = DocumentProcessor._extract_arxiv_id(source)\n",
    "                if arxiv_id:\n",
    "                    loader = ArxivLoader(query=arxiv_id, load_max_docs=1)\n",
    "                else:\n",
    "                    loader = WebBaseLoader(source)\n",
    "            elif source.startswith(\"http\"):\n",
    "                loader = WebBaseLoader(source)\n",
    "            elif source.startswith(\"arxiv:\"):\n",
    "                # Handle direct arXiv IDs \n",
    "                arxiv_id = source.replace(\"arxiv:\", \"\")\n",
    "                loader = ArxivLoader(query=arxiv_id, load_max_docs=1)\n",
    "            elif os.path.isfile(source):\n",
    "                file_extension = os.path.splitext(source)[1].lower()\n",
    "                if file_extension == \".pdf\":\n",
    "                    # Use PyPDFLoader for PDF files \n",
    "                    logging.info(f\"Detected PDF file, using PyPDFLoader for {source}\")\n",
    "                    loader = PyPDFLoader(source)\n",
    "            else:\n",
    "                # Raise an error if the source type is not recognized or file doesn't exist\n",
    "                raise ValueError(f\"Unsupported document source or file not found: {source}\")\n",
    "\n",
    "            documents = loader.load()\n",
    "\n",
    "            if not documents:\n",
    "                raise ValueError(\"No documents loaded from the specified source.\")\n",
    "            \n",
    "            logger.info(f\"Successfully loaded {len(documents)} document(s) from {source}\")\n",
    "            return documents\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"File not found error while loading document: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred during document loading: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_arxiv_id(url):\n",
    "        \"\"\" Extract arXiv ID from an arXiv URL\"\"\"\n",
    "        patterns = [\n",
    "            r'arxiv\\.org/abs/(\\d+\\.\\d+)',  # e.g., https://arxiv.org/abs/1234.5678\n",
    "            r'arxiv\\.org/pdf/(\\d+\\.\\d+)'   # e.g., https://arxiv.org/pdf/1234.5678\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_documents(documents):\n",
    "        \"\"\" Cleans and preprocesses a list of LangChain Document object\"\"\"\n",
    "\n",
    "        logger.info(\"Starting document preprocessing...\")\n",
    "\n",
    "        processed_docs = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            content = doc.page_content\n",
    "\n",
    "            # Remove excessive whitespace \n",
    "            content = re.sub(r'\\\\s+',' ', content).strip()\n",
    "\n",
    "            # Remove common artifacts like isolated page numbers \n",
    "            # Use regex to look for a newline, optional whitespace, digits, optional whitespace, and another newline\n",
    "            content = re.sub(r'\\\\n\\\\s*\\\\d+\\\\s*\\\\n', '\\\\n', content)\n",
    "\n",
    "        # Filter out documents with very short content after cleaning\n",
    "            if len(content) < 100:\n",
    "                logger.warning(f\"Skipping document {i} due to very short content after preprocessing (length: {len(content)}).\")\n",
    "                continue\n",
    "\n",
    "            # Create a new Document object with cleaned content\n",
    "            processed_doc = Document(page_content=content, metadata=doc.metadata)\n",
    "            processed_docs.append(processed_doc)\n",
    "\n",
    "        logger.info(f\"Finished preprocessing. Original documents: {len(documents)}, Preprocessed documents: {len(processed_docs)}\")\n",
    "        return processed_docs\n",
    "\n",
    "print(\"DocumentProcessor class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2930b",
   "metadata": {},
   "source": [
    "### 3. Create PersonaPrompts Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ceddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersonaPrompts class defined.\n"
     ]
    }
   ],
   "source": [
    "class PersonaPrompts:\n",
    "    \"\"\" Manages persona-specific prompts\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_personas():\n",
    "        \"\"\" Define different persona descriptions\"\"\"\n",
    "        return {\n",
    "            \"Data Scientist\": (\"a data scientist with strong knowledge in machine learning and deep learning who is currently learning generative AI. \"\n",
    "                \"Focus on practical applications, data requirements, model performance metrics, implementation considerations, \"\n",
    "                \"and how this relates to traditional ML/DL approaches you already know. Use technical language but explain generative AI concepts clearly.\"),\n",
    "            \"AI Engineer\": (\n",
    "                \"a senior AI engineer responsible for implementing and deploying ML systems. \"\n",
    "                \"Focus on practical implementation details, computational requirements, scalability considerations, \"\n",
    "                \"integration challenges, and performance metrics. Use engineering-focused language.\"\n",
    "            ),\n",
    "            \"Graduate Student\": (\n",
    "                \"a graduate student studying machine learning who needs to understand this paper for research. \"\n",
    "                \"Explain the core problem, methodology, key findings, and significance. \"\n",
    "                \"Use clear technical language but explain complex concepts.\"\n",
    "            ),\n",
    "            \"Business Executive\": (\n",
    "                \"a business executive with limited technical background who needs to understand the business impact. \"\n",
    "                \"Focus on the problem being solved, potential applications, market implications, \"\n",
    "                \"competitive advantages, and ROI considerations. Avoid technical jargon.\"\n",
    "            ),\n",
    "            \"General Audience\": (\n",
    "                \"explaining to an educated general audience with no AI background. \"\n",
    "                \"Use simple language, analogies, and focus on the big picture: what problem is solved, \"\n",
    "                \"how it works in simple terms, and why it matters.\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_prompt_templates():\n",
    "        \"\"\"Create persona-specific prompt templates.\"\"\"\n",
    "        personas = PersonaPrompts.get_personas()\n",
    "        templates = {}\n",
    "        \n",
    "        base_template = \"\"\"You are {persona_description}\n",
    "\n",
    "                            Based on the following research paper content, provide a comprehensive summary that addresses:\n",
    "                            1. The main problem or research question\n",
    "                            2. The proposed approach/solution\n",
    "                            3. Key findings and results \n",
    "                            4. Significance and implications\n",
    "                            5. Limitations or future work (if mentioned)\n",
    "\n",
    "                            Keep your summary focused on aspects most relevant to your perspective and audience.\n",
    "\n",
    "                            Research Paper Content:\n",
    "                            {context}\n",
    "\n",
    "                            Summary:\"\"\"\n",
    "        \n",
    "        for name, description in personas.items():\n",
    "            templates[name] =  ChatPromptTemplate.from_template(base_template).partial(persona_description=description)\n",
    "        \n",
    "        return templates\n",
    "\n",
    "print(\"PersonaPrompts class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25557ca5",
   "metadata": {},
   "source": [
    "### 4. Create SummaryEvaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc927cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummaryEvaluator class defined with model model='claude-sonnet-4-20250514' temperature=0.5 anthropic_api_url='https://api.anthropic.com' anthropic_api_key=SecretStr('**********') model_kwargs={}.\n"
     ]
    }
   ],
   "source": [
    "class SummaryEvaluator:\n",
    "    \"\"\"Evaluates summary quality using LLM-as-a-judge.\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.eval_prompt = self._create_evaluation_prompt()\n",
    "\n",
    "    def _create_evaluation_prompt(self):\n",
    "        \"\"\"Create evaluation prompt template.\"\"\"\n",
    "        template = \"\"\"You are an expert evaluator assessing the quality for research paper summaries.\n",
    "\n",
    "                        Evaluate the following summary based on these criteria:\n",
    "                        1. Accuracy: Does it correctly represent the source material?\n",
    "                        2. Completeness: Does it cover the key points appropriately?\n",
    "                        3. Clarity: Is it well-written and understandable for the target audience?\n",
    "                        4. Relevance: Does it focus on aspects relevant to the specified persona?\n",
    "\n",
    "                        Rate the summary on a scale of 1-5 where:\n",
    "                        1 = Poor (major inaccuracies, missing key points, unclear)\n",
    "                        2 = Fair (some issues with accuracy or completeness)\n",
    "                        3 = Good (mostly accurate and complete, minor issues)\n",
    "                        4 = Very Good (accurate, complete, well-written)\n",
    "                        5 = Excellent (outstanding in all criteria)\n",
    "\n",
    "                        Provide your rating and detailed justification.\n",
    "\n",
    "                        Source Material:\n",
    "                        {context}\n",
    "\n",
    "                        Summary to Evaluate:\n",
    "                        {summary}\n",
    "\n",
    "                        Persona: {persona}\n",
    "\n",
    "                        Evaluation:\"\"\"\n",
    "        return ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    def evaluate(self, summary, context, persona):\n",
    "        \"\"\" Evaluate a single summary.\"\"\"\n",
    "        try:\n",
    "            chain = self.eval_prompt | self.llm | StrOutputParser()\n",
    "            evaluation = chain.invoke({\n",
    "                \"summary\": summary,\n",
    "                \"context\": context,\n",
    "                \"persona\": persona\n",
    "            })\n",
    "            return evaluation\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating summary: {e}\")\n",
    "            return f\"Evaluation failed: {str(e)}\"\n",
    "\n",
    "print(f\"SummaryEvaluator class defined with model {default_llm_eval}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c008d3",
   "metadata": {},
   "source": [
    "### 5. Create PaperSummarizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93088a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaperSummarizer class defined\n"
     ]
    }
   ],
   "source": [
    "class PaperSummarizer:\n",
    "    \"\"\" Main class responsible for the summarization process.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, window_size=2):\n",
    "        self.llm = default_llm\n",
    "        self.embeddings = embeddings_model\n",
    "        self.retriever = SentenceWindowRetriever(window_size=window_size)\n",
    "        self.persona_prompts = PersonaPrompts.create_prompt_templates()\n",
    "        self.evaluator = SummaryEvaluator(default_llm_eval)\n",
    "\n",
    "        logger.info(f\"Initialized PaperSummarizer with model {llm_model}\")\n",
    "\n",
    "\n",
    "    def process_document(self, source):\n",
    "        \"\"\" Load and process document, return processed documents and context.\"\"\"\n",
    "        # Load document\n",
    "        documents = DocumentProcessor.load_documents(source)\n",
    "\n",
    "        # Preprocess\n",
    "        processed_docs = DocumentProcessor.preprocess_documents(documents)\n",
    "\n",
    "        if not processed_docs:\n",
    "            raise ValueError(\"No valid documents after preprocessing.\")\n",
    "        \n",
    "        # Create sentence windows\n",
    "        windows = self.retriever.create_sentence_windows(processed_docs)\n",
    "\n",
    "        # Build vectorstore\n",
    "        self.retriever.build_vectorstore(self.embeddings)\n",
    "\n",
    "        return processed_docs, f\"Processed {len(windows)} sentence windows\"\n",
    "    \n",
    "\n",
    "    def generate_summaries(self, query=\"Summarize this research paper\"):\n",
    "        \"\"\"Generate summaries for all personas.\"\"\"\n",
    "        # Retrieve relevant context\n",
    "        retrieved_docs = self.retriever.retrieve(query, k=150) # Get more context\n",
    "        context = \"/n/n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        if not context.strip():\n",
    "            raise ValueError(\"No relevant context retrieved\")\n",
    "        \n",
    "        logger.info(f\"Retrieved context length: {len(context)} characters\")\n",
    "\n",
    "        # Generate summaries for each persona\n",
    "        summaries = {}\n",
    "        for persona, template in self.persona_prompts.items():\n",
    "            try:\n",
    "                logger.info(f\"Generating summary for: {persona}\")\n",
    "\n",
    "                chain = template | self.llm | StrOutputParser()\n",
    "                summary = chain.invoke({\"context\": context})\n",
    "                summaries[persona] = summary\n",
    "\n",
    "                # Add a small delay to avoid hitting API rate limits\n",
    "                time.sleep(30)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating summary for {persona}: {e}\")\n",
    "                summaries[persona] = f\"Error generating summary: {str(e)}\"\n",
    "        \n",
    "        return summaries, context\n",
    "    \n",
    "\n",
    "    def evaluate_summaries(self, summaries, context):\n",
    "        \"\"\" Evaluate all generated summaries.\"\"\"\n",
    "        evaluations = {}\n",
    "\n",
    "        for persona_name, summary in summaries.items():\n",
    "            logger.info(f\"Evaluating summary for {persona_name}\")\n",
    "            evaluation = self.evaluator.evaluate(summary, context, persona_name)\n",
    "            evaluations[persona_name] = evaluation\n",
    "            \n",
    "            # Add a small delay to avoid hitting API rate limits\n",
    "            time.sleep(30)\n",
    "        \n",
    "        return evaluations\n",
    "    \n",
    "\n",
    "    def summarize_paper(self, source, query = \"Summarize this research paper\"):\n",
    "        \"\"\"Complete process: load, preprocess, summarize, and evaluate.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting paper summarizarion for {source}\")\n",
    "\n",
    "            # Process document\n",
    "            processed_docs, processing_info = self.process_document(source)\n",
    "\n",
    "            # Generate summaries\n",
    "            summaries, context = self.generate_summaries(query)\n",
    "\n",
    "            # Evaluate summaries\n",
    "            evaluations = self.evaluate_summaries(summaries, context)\n",
    "\n",
    "            # Compile results\n",
    "            results = {\n",
    "                \"source\": source,\n",
    "                \"query\": query,\n",
    "                \"processing_info\": processing_info,\n",
    "                \"context_length\": len(context),\n",
    "                \"summaries\": summaries,\n",
    "                \"evaluations\": evaluations,\n",
    "                \"personas\": list(summaries.keys())\n",
    "            }\n",
    "\n",
    "            logger.info(\"Paper summarization completed successfully.\")\n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in summarization process: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"PaperSummarizer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143fa8e",
   "metadata": {},
   "source": [
    "### 6. Create get_paper_summary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8531b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_paper_summary function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_paper_summary(source:str, query:str = \"Summarize this research paper\") -> dict:\n",
    "    \"\"\" Summarizes a research paper from a given source.\"\"\"\n",
    "    try:\n",
    "        summarizer = PaperSummarizer(default_llm)\n",
    "        results = summarizer.summarize_paper(source, query)\n",
    "        return results\n",
    "    except ValueError as ve:\n",
    "        logger.error(f\"Configuration error: {ve}\")\n",
    "        return {\"error\": str(ve)}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "print(\"get_paper_summary function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481a5fa",
   "metadata": {},
   "source": [
    "## 3. Generating Summaries for Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77b4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized PaperSummarizer with model claude-3-7-sonnet-20250219\n",
      "INFO:__main__:Starting paper summarizarion for Attention is All You Need.pdf\n",
      "INFO:__main__:Loading document from Attention is All You Need.pdf\n",
      "INFO:root:Detected PDF file, using PyPDFLoader for Attention is All You Need.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to summarize Attention is All You Need.pdf with model claude-3-7-sonnet-20250219 and query 'Summarize this research paper'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 15 document(s) from Attention is All You Need.pdf\n",
      "INFO:__main__:Starting document preprocessing...\n",
      "INFO:__main__:Finished preprocessing. Original documents: 15, Preprocessed documents: 15\n",
      "INFO:__main__:Creating sentence windows....\n",
      "INFO:__main__:Created 373 sentence windows.\n",
      "INFO:__main__:Building vector store from sentence windows...\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "INFO:__main__:Vector store built successfully.\n",
      "INFO:__main__:Retrieved context length: 61788 characters\n",
      "INFO:__main__:Generating summary for: Data Scientist\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: AI Engineer\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: Graduate Student\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: Business Executive\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: General Audience\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for Data Scientist\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for AI Engineer\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for Graduate Student\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for Business Executive\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for General Audience\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Paper summarization completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PAPER SUMMARIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Source: Attention is All You Need.pdf\n",
      "Query: Summarize this research paper\n",
      "LLM Model Used: claude-3-7-sonnet-20250219\n",
      "Processing: Processed 373 sentence windows\n",
      "Context Length: 61,788 characters\n",
      "\n",
      "------------------------------------------------------------\n",
      "PERSONA SUMMARIES\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- DATA SCIENTIST ---\n",
      "# Comprehensive Summary of \"Attention Is All You Need\"\n",
      "\n",
      "## 1. Main Problem/Research Question\n",
      "The paper addresses limitations in sequence transduction models that rely on complex recurrent or convolutional neural networks with encoder-decoder architectures. Traditional models face challenges in learning long-distance dependencies and have limited parallelization capabilities during training. The authors investigate whether attention mechanisms alone can create effective sequence models without using recurrence or convolution.\n",
      "\n",
      "## 2. Proposed Approach/Solution\n",
      "The authors introduce the Transformer, a novel neural network architecture based entirely on attention mechanisms:\n",
      "\n",
      "- **Architecture**: Encoder-decoder structure with stacked self-attention and point-wise fully connected layers\n",
      "- **Key Components**:\n",
      "  - **Multi-Head Attention**: Projects queries, keys, and values h times with different learned projections, allowing the model to jointly attend to information from different representation subspaces\n",
      "  - **Scaled Dot-Product Attention**: Computes attention by taking dot products of queries with keys, scaling by 1/√dk, and applying softmax to obtain weights for values\n",
      "  - **Position-wise Feed-Forward Networks**: Applied to each position separately and identically\n",
      "  - **Positional Encoding**: Adds information about token position in the sequence\n",
      "  - **Residual Connections and Layer Normalization**: Used around each sub-layer\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "- The Transformer outperforms previous state-of-the-art models on machine translation tasks:\n",
      "  - Achieved 28.4 BLEU score on WMT 2014 English-to-German translation, exceeding previous best models by more than 2.0 BLEU points\n",
      "  - Achieved 41.0 BLEU score on WMT 2014 English-to-French translation\n",
      "- The model requires significantly less training time compared to RNN/CNN-based architectures\n",
      "- Multi-head attention is more effective than single-head attention, with 8 heads being optimal in their experiments\n",
      "- Attention key size (dk) is important for model quality, suggesting compatibility determination is non-trivial\n",
      "- Visualization of attention heads shows they learn distinct and interpretable behaviors, with some heads capturing syntactic and semantic relationships\n",
      "\n",
      "## 4. Significance and Implications\n",
      "- Demonstrates that self-attention can replace recurrence and convolution entirely in sequence transduction tasks\n",
      "- Enables significantly more parallelization during training, reducing training time\n",
      "- Provides constant-time path length between any two positions in the sequence, addressing the challenge of learning long-range dependencies\n",
      "- Creates more interpretable models, as attention distributions can be visualized to understand what the model is focusing on\n",
      "- Establishes a new architecture paradigm that has since become foundational for many subsequent advances in NLP and beyond\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "- The model has reduced effective resolution due to averaging attention-weighted positions, which is partially addressed by multi-head attention\n",
      "- The authors suggest that more sophisticated compatibility functions than dot product may be beneficial\n",
      "- The paper notes that self-attention could yield more interpretable models, but this area requires further exploration\n",
      "- While not explicitly stated as future work, the visualizations suggest potential for deeper analysis of how different attention heads learn to perform specialized linguistic functions\n",
      "\n",
      "From a data science perspective, the Transformer architecture represents a significant advancement in how we can model sequential data, offering better parallelization, improved handling of long-range dependencies, and greater interpretability compared to traditional RNN/CNN approaches. The multi-head attention mechanism provides a powerful way to capture different types of relationships in the data simultaneously, which is particularly valuable for complex language understanding tasks.\n",
      "\n",
      "--- AI ENGINEER ---\n",
      "# Engineering Analysis: \"Attention Is All You Need\" Paper\n",
      "\n",
      "## 1. Main Problem/Research Question\n",
      "The paper addresses limitations in sequence transduction models that rely on complex recurrent or convolutional neural networks with encoder-decoder architectures. Traditional approaches face challenges with long-range dependencies and limited parallelization capabilities, creating computational bottlenecks in training and inference pipelines.\n",
      "\n",
      "## 2. Proposed Approach/Solution\n",
      "The authors introduce the Transformer architecture, which completely eliminates recurrence and convolutions in favor of self-attention mechanisms. Key technical components include:\n",
      "\n",
      "- **Multi-head attention**: Projects queries, keys, and values h times with different learned projections, allowing the model to jointly attend to information from different representation subspaces\n",
      "- **Scaled dot-product attention**: Computes attention weights using dot products scaled by 1/√dk to prevent gradient vanishing in deeper layers\n",
      "- **Position-wise feed-forward networks**: Applied to each position separately and identically\n",
      "- **Residual connections and layer normalization**: Used around each sub-layer to facilitate gradient flow\n",
      "- **Positional encodings**: Added to input embeddings to retain sequence order information\n",
      "\n",
      "The architecture consists of encoder and decoder stacks (6 layers each in the base model), with attention mechanisms handling three distinct functions: encoder self-attention, decoder self-attention, and encoder-decoder attention.\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "- The Transformer architecture achieved state-of-the-art results on machine translation benchmarks:\n",
      "  - 28.4 BLEU on WMT 2014 English-to-German (2.0 BLEU improvement over previous SOTA)\n",
      "  - 41.0 BLEU on WMT 2014 English-to-French\n",
      "- Training efficiency was significantly improved, requiring only a fraction of the computational resources compared to previous approaches\n",
      "- Ablation studies showed that:\n",
      "  - Multi-head attention outperforms single-head attention (optimal at 8 heads)\n",
      "  - Attention key dimension size is critical for model quality\n",
      "  - Model scaling (increasing dimensions) consistently improves performance\n",
      "\n",
      "## 4. Significance and Implications\n",
      "From an engineering perspective, the Transformer architecture represents a paradigm shift in sequence modeling with several practical advantages:\n",
      "\n",
      "- **Parallelization**: By removing sequential dependencies in computation, the model enables much higher throughput during training on modern hardware accelerators\n",
      "- **Computational efficiency**: Constant number of operations to relate signals between positions, regardless of sequence length\n",
      "- **Interpretability**: Attention distributions provide insights into model behavior, with different heads learning distinct and interpretable functions\n",
      "- **Scalability**: The architecture scales effectively with more parameters and compute\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "- The model's effective resolution is reduced due to averaging attention-weighted positions\n",
      "- More sophisticated compatibility functions than dot product may be beneficial for attention mechanisms\n",
      "- The paper notes that determining compatibility is not trivial, suggesting room for improvement\n",
      "- While not explicitly stated as a limitation, the model's quadratic complexity with sequence length (O(n²)) could become problematic for very long sequences\n",
      "\n",
      "This architecture has become foundational for modern NLP systems, with implications for deployment architectures, hardware requirements, and scaling strategies in production ML systems.\n",
      "\n",
      "--- GRADUATE STUDENT ---\n",
      "# Attention Is All You Need: A Comprehensive Summary\n",
      "\n",
      "## 1. Main Problem and Research Question\n",
      "\n",
      "The paper \"Attention Is All You Need\" addresses a fundamental challenge in sequence transduction tasks like machine translation: the reliance on complex recurrent or convolutional neural networks with encoder-decoder architectures. Traditional approaches faced limitations in modeling long-range dependencies and were difficult to parallelize. The authors investigate whether attention mechanisms alone could replace recurrent and convolutional components entirely while maintaining or improving performance.\n",
      "\n",
      "## 2. Proposed Approach/Solution\n",
      "\n",
      "The authors introduce the **Transformer**, a novel neural network architecture that relies solely on attention mechanisms without using recurrence or convolution. Key components include:\n",
      "\n",
      "- **Self-attention**: Allows the model to relate different positions in a sequence directly\n",
      "- **Multi-head attention**: Projects queries, keys, and values into different representation subspaces, enabling the model to attend to information from different positions simultaneously\n",
      "- **Scaled dot-product attention**: A computationally efficient attention mechanism that scales dot products by 1/√d_k to prevent extremely small gradients\n",
      "- **Position-wise feed-forward networks**: Applied to each position separately and identically\n",
      "- **Positional encodings**: Added to input embeddings to provide information about token position\n",
      "\n",
      "The architecture consists of an encoder and decoder, each composed of 6 identical layers with residual connections and layer normalization. The encoder uses self-attention, while the decoder uses both masked self-attention (to prevent attending to future positions) and encoder-decoder attention.\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "\n",
      "The Transformer achieved state-of-the-art results on machine translation tasks:\n",
      "\n",
      "- On the WMT 2014 English-to-German translation task, the large Transformer model achieved a BLEU score of 28.4, outperforming previous models by more than 2.0 BLEU points\n",
      "- On the WMT 2014 English-to-French translation task, it achieved a BLEU score of 41.0\n",
      "- Even the base Transformer model surpassed all previously published models at a fraction of the training cost\n",
      "- Ablation studies showed that multi-head attention was crucial (8 heads performed best), and reducing attention key size hurt model quality\n",
      "\n",
      "## 4. Significance and Implications\n",
      "\n",
      "The Transformer architecture represents a paradigm shift in sequence modeling by:\n",
      "\n",
      "1. Demonstrating that attention mechanisms alone can achieve superior performance without recurrence or convolution\n",
      "2. Enabling significantly more parallelization during training, reducing training time substantially\n",
      "3. Providing a constant number of operations to relate signals between positions, regardless of distance, making it easier to learn long-range dependencies\n",
      "4. Creating more interpretable models where different attention heads learn to perform distinct tasks related to syntactic and semantic structure\n",
      "5. Establishing a foundation for future work in NLP that would eventually lead to models like BERT, GPT, and other transformer-based architectures\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "\n",
      "While not extensively discussed in the paper, some limitations can be inferred:\n",
      "\n",
      "- The constant path length between positions comes at the cost of reduced effective resolution due to averaging attention-weighted positions\n",
      "- The model requires positional encodings since it lacks the sequential inductive bias of RNNs\n",
      "- The compatibility function (dot product) may not be optimal, as experiments showed that reducing attention key size hurt performance\n",
      "- The quadratic complexity of self-attention with respect to sequence length could be problematic for very long sequences\n",
      "\n",
      "The authors note that self-attention could yield more interpretable models and provide visualizations showing how different attention heads learn to perform different tasks, suggesting a direction for future research into model interpretability.\n",
      "\n",
      "As a graduate student in machine learning, this paper is particularly significant as it introduced a fundamental architecture that has become the backbone of most state-of-the-art NLP systems today.\n",
      "\n",
      "--- BUSINESS EXECUTIVE ---\n",
      "# Business Executive Summary: \"Attention Is All You Need\"\n",
      "\n",
      "## 1. The Main Problem\n",
      "Traditional machine translation and language processing systems rely on complex recurrent or convolutional neural networks that are difficult to train, computationally expensive, and struggle with long-distance relationships in text. These limitations create bottlenecks in developing more efficient and accurate language processing applications.\n",
      "\n",
      "## 2. The Proposed Solution\n",
      "The researchers introduced a revolutionary new architecture called the \"Transformer\" that completely eliminates the need for recurrent networks or convolutions. Instead, it relies entirely on a mechanism called \"self-attention\" that allows the model to focus on relevant parts of input text regardless of their distance from each other. This approach makes the model both more powerful and more efficient.\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "- The Transformer model achieved breakthrough performance on machine translation tasks, outperforming previous state-of-the-art systems by more than 2.0 BLEU points (a standard quality metric)\n",
      "- The model required significantly less training time than competitors - achieving superior results with just a fraction of the computational resources\n",
      "- The attention mechanism demonstrated an ability to capture meaningful linguistic relationships in text, including long-distance dependencies\n",
      "\n",
      "## 4. Business Significance and Implications\n",
      "- **Cost Efficiency**: The Transformer architecture enables faster training and inference with fewer computational resources, potentially reducing infrastructure costs for language AI applications\n",
      "- **Quality Improvements**: The significant performance gains translate to more accurate translations and better language understanding capabilities\n",
      "- **Competitive Advantage**: Organizations implementing this technology could deliver superior language processing applications while using fewer resources\n",
      "- **Versatility**: While demonstrated on translation, the architecture appears applicable to many language tasks including summarization, question answering, and text generation\n",
      "- **Interpretability**: The attention mechanism provides insights into how the model makes decisions, potentially making it more transparent than previous approaches\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "The research acknowledges that while the attention mechanism is powerful, determining compatibility between different parts of text is challenging, suggesting that more sophisticated approaches may yield further improvements. The researchers also experimented with various model configurations, indicating that continued refinement of the architecture could lead to additional performance gains.\n",
      "\n",
      "This breakthrough represents a fundamental shift in how language processing systems can be built, offering a more efficient and effective foundation for a wide range of business applications involving language understanding and generation.\n",
      "\n",
      "--- GENERAL AUDIENCE ---\n",
      "# Attention Is All You Need: Transforming Language Processing\n",
      "\n",
      "## The Main Problem\n",
      "\n",
      "Traditional machine translation and language processing systems relied on complex recurrent neural networks (RNNs) or convolutional neural networks (CNNs) that processed text sequentially. These approaches had two major limitations:\n",
      "\n",
      "1. They couldn't easily process long sentences because information had to flow through many sequential steps\n",
      "2. They couldn't be parallelized efficiently, making training slow and computationally expensive\n",
      "\n",
      "## The Proposed Solution: The Transformer\n",
      "\n",
      "The researchers introduced a revolutionary new architecture called the \"Transformer\" that completely eliminates recurrence and convolutions. Instead, it relies entirely on a mechanism called \"self-attention\" to process language.\n",
      "\n",
      "Think of self-attention like this: when you read a sentence, you naturally pay attention to different words at different times to understand meaning. For example, in \"The cat, which was gray, ran across the street,\" you connect \"cat\" with \"ran\" despite the words between them. The Transformer mimics this ability by allowing each word to directly \"look at\" and connect with any other word in the sentence.\n",
      "\n",
      "The key innovations include:\n",
      "\n",
      "1. **Multi-head attention**: Instead of having a single attention mechanism, the model uses multiple \"attention heads\" that each focus on different aspects of the relationships between words\n",
      "2. **Scaled dot-product attention**: A mathematical technique that helps the model efficiently determine which words should pay attention to each other\n",
      "3. **Positional encoding**: Since the model processes all words simultaneously (not sequentially), it needs a way to understand word order\n",
      "\n",
      "## Key Findings and Results\n",
      "\n",
      "The Transformer achieved breakthrough results:\n",
      "\n",
      "- It outperformed all previous translation systems by more than 2.0 BLEU points (a standard measure of translation quality) on English-to-German translation\n",
      "- It achieved state-of-the-art results on English-to-French translation\n",
      "- It required significantly less training time than previous approaches (hours instead of days)\n",
      "- The researchers found that different attention heads learned to perform different linguistic tasks automatically - some focused on syntactic relationships, others on semantic connections\n",
      "\n",
      "## Significance and Implications\n",
      "\n",
      "This paper fundamentally changed how we approach language processing:\n",
      "\n",
      "1. **Efficiency**: Transformers can be trained in parallel, dramatically reducing training time\n",
      "2. **Scalability**: The architecture can be scaled to much larger models and datasets\n",
      "3. **Versatility**: The approach works for many language tasks beyond translation\n",
      "4. **Interpretability**: The attention patterns provide insights into how the model understands language\n",
      "\n",
      "The Transformer architecture became the foundation for virtually all modern language AI systems, including BERT, GPT, and other large language models that power today's AI applications.\n",
      "\n",
      "## Limitations and Future Work\n",
      "\n",
      "The researchers noted some limitations:\n",
      "\n",
      "- The model's effectiveness depends on having multiple attention heads (performance drops with too few or too many)\n",
      "- The attention mechanism's \"compatibility function\" (how it determines which words should attend to each other) could potentially be improved\n",
      "- The model trades computational efficiency for some reduction in resolution when processing very long sequences\n",
      "\n",
      "This work opened the door to a new era of language AI research that continues to evolve today.\n",
      "\n",
      "------------------------------------------------------------\n",
      "SUMMARY EVALUATIONS\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- DATA SCIENTIST EVALUATION ---\n",
      "**Rating: 4 (Very Good)**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4.5/5)\n",
      "The summary accurately represents the key technical concepts from the Transformer paper. It correctly describes:\n",
      "- The multi-head attention mechanism and scaled dot-product attention\n",
      "- The architectural components (encoder-decoder structure, residual connections, layer normalization)\n",
      "- The specific BLEU scores achieved (28.4 for English-German, 41.0 for English-French)\n",
      "- The experimental findings about optimal number of heads (8) and importance of attention key size\n",
      "\n",
      "Minor accuracy issue: The summary states the model uses \"h times\" projections but could be clearer that h=8 specifically in their implementation.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all major aspects of the paper comprehensively:\n",
      "- Problem motivation and research question\n",
      "- Technical architecture details\n",
      "- Experimental results and comparisons\n",
      "- Significance and implications\n",
      "- Limitations and future directions\n",
      "\n",
      "The summary effectively captures both the technical innovations and their practical impact. It could have included slightly more detail about the training setup (WMT datasets, hardware requirements) but covers the essential elements well.\n",
      "\n",
      "### Clarity (4.5/5)\n",
      "The summary is exceptionally well-written for a data scientist audience:\n",
      "- Uses appropriate technical terminology while remaining accessible\n",
      "- Organizes information logically with clear section headers\n",
      "- Explains complex concepts like multi-head attention in understandable terms\n",
      "- Provides specific quantitative results that data scientists value\n",
      "- Concludes with practical implications for modeling sequential data\n",
      "\n",
      "### Relevance (4/5)\n",
      "The summary is highly relevant to a data scientist persona:\n",
      "- Emphasizes practical advantages (parallelization, training efficiency)\n",
      "- Discusses model interpretability, which is crucial for data scientists\n",
      "- Frames the work in terms of modeling sequential data broadly\n",
      "- Highlights the significance for handling long-range dependencies\n",
      "- Concludes with actionable insights about the architecture's value\n",
      "\n",
      "The final paragraph particularly well connects the technical innovations to broader data science applications beyond just NLP.\n",
      "\n",
      "**Overall Assessment:**\n",
      "This is a high-quality summary that successfully distills a complex technical paper into an accessible and comprehensive overview. It maintains technical accuracy while being appropriately tailored for a data scientist audience, emphasizing practical implications and model capabilities that would be most relevant to practitioners in the field.\n",
      "\n",
      "--- AI ENGINEER EVALUATION ---\n",
      "**Rating: 4 (Very Good)**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4/5)\n",
      "The summary accurately represents the core technical concepts from the Transformer paper. Key details are correct:\n",
      "- Multi-head attention mechanism and scaled dot-product attention are properly described\n",
      "- Architecture components (encoder-decoder stacks, 6 layers each) are accurate\n",
      "- Performance results (28.4 BLEU English-German, 41.0 BLEU English-French) are correctly stated\n",
      "- Technical specifications (8 attention heads, scaling factor 1/√dk) are precise\n",
      "\n",
      "Minor issue: The summary mentions \"gradient vanishing in deeper layers\" as the reason for scaling, but the source material specifically discusses softmax saturation due to large dot products, not gradient vanishing per se.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all major aspects relevant to an AI engineer:\n",
      "- Problem motivation and limitations of existing approaches\n",
      "- Detailed technical architecture description\n",
      "- Experimental results and ablation studies\n",
      "- Engineering implications (parallelization, efficiency, scalability)\n",
      "- Limitations and future directions\n",
      "\n",
      "The summary appropriately balances technical depth with readability, though it could have mentioned the training details (WMT datasets, hardware setup with 8 P100 GPUs) which are relevant for engineering implementation.\n",
      "\n",
      "### Clarity (4/5)\n",
      "Very well-written and structured for the target audience. The summary:\n",
      "- Uses appropriate technical terminology for AI engineers\n",
      "- Organizes information logically with clear sections\n",
      "- Explains complex concepts (multi-head attention, positional encodings) concisely\n",
      "- Maintains good flow between sections\n",
      "\n",
      "The writing is professional and accessible to practitioners who need to understand implementation details.\n",
      "\n",
      "### Relevance (5/5)\n",
      "Excellently tailored for an AI engineer persona:\n",
      "- Emphasizes practical engineering considerations (parallelization, computational efficiency, hardware requirements)\n",
      "- Focuses on architectural details and implementation aspects\n",
      "- Discusses scalability and deployment implications\n",
      "- Includes performance metrics and ablation study results that inform engineering decisions\n",
      "- Addresses limitations from a practical implementation perspective\n",
      "\n",
      "The summary successfully bridges the gap between research findings and engineering applications, making it highly valuable for practitioners working on similar systems.\n",
      "\n",
      "**Overall Assessment:**\n",
      "This is a strong summary that effectively communicates the technical innovations and practical implications of the Transformer architecture to AI engineers. The minor accuracy issue and slight gaps in completeness prevent it from achieving a perfect score, but it demonstrates excellent understanding of both the source material and the target audience's needs.\n",
      "\n",
      "--- GRADUATE STUDENT EVALUATION ---\n",
      "**Rating: 4 (Very Good)**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4/5)\n",
      "The summary accurately represents the core concepts and findings from the Transformer paper. Key technical details are correctly presented, including:\n",
      "- Proper description of scaled dot-product attention with the 1/√dk scaling factor\n",
      "- Accurate reporting of BLEU scores (28.4 for EN-DE, 41.0 for EN-FR)\n",
      "- Correct architectural details (6 layers, 8 attention heads, etc.)\n",
      "- Accurate explanation of multi-head attention mechanisms\n",
      "\n",
      "Minor issue: The summary mentions some limitations that are somewhat inferred rather than explicitly stated in the source material, though these inferences are reasonable.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all major aspects of the paper comprehensively:\n",
      "- Problem motivation and research question\n",
      "- Detailed architectural components\n",
      "- Experimental results and ablation studies\n",
      "- Significance and broader implications\n",
      "- Technical innovations like positional encodings and layer normalization\n",
      "\n",
      "The structure follows a logical flow that captures both technical depth and broader context. Only minor implementation details are omitted, which is appropriate for the target audience.\n",
      "\n",
      "### Clarity (5/5)\n",
      "Exceptionally well-written for a graduate student audience. The summary:\n",
      "- Uses appropriate technical terminology while remaining accessible\n",
      "- Provides clear explanations of complex concepts like multi-head attention\n",
      "- Maintains good organization with clear section headings\n",
      "- Balances technical detail with conceptual understanding\n",
      "- Includes specific numbers and metrics that graduate students would find valuable\n",
      "\n",
      "### Relevance (4/5)\n",
      "Highly relevant to a graduate student persona:\n",
      "- Emphasizes the paradigm shift and foundational importance for future research\n",
      "- Connects to broader implications for the field (mentioning BERT, GPT)\n",
      "- Discusses interpretability aspects that would interest ML researchers\n",
      "- Provides technical depth appropriate for someone conducting research\n",
      "- Concludes with a personal note about significance for graduate study\n",
      "\n",
      "The summary successfully bridges technical understanding with research context, making it valuable for someone who needs to understand both the mechanics and the broader impact of this work.\n",
      "\n",
      "**Overall Assessment:** This is a very strong summary that demonstrates deep understanding of the source material and effectively communicates it to the target audience. The minor accuracy concerns don't significantly detract from its overall quality and usefulness for a graduate student in machine learning.\n",
      "\n",
      "--- BUSINESS EXECUTIVE EVALUATION ---\n",
      "**Rating: 4 - Very Good**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4.5/5)\n",
      "The summary accurately represents the core contributions of the Transformer paper. Key technical details are correctly captured:\n",
      "- Proper identification of the main innovation (self-attention replacing RNNs/CNNs)\n",
      "- Accurate reporting of performance gains (2.0+ BLEU improvement)\n",
      "- Correct emphasis on computational efficiency benefits\n",
      "- Accurate description of the attention mechanism's capabilities\n",
      "\n",
      "Minor deduction for slightly oversimplifying some technical nuances, but this is appropriate for the executive audience.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all essential points for a business executive:\n",
      "- Problem statement and market context\n",
      "- Technical solution overview\n",
      "- Quantified results and performance metrics\n",
      "- Business implications and strategic value\n",
      "- Limitations and future considerations\n",
      "\n",
      "The structure effectively balances technical understanding with business relevance. Could have included more specific details about training time comparisons or computational resource savings.\n",
      "\n",
      "### Clarity (4.5/5)\n",
      "Exceptionally well-written for the target audience:\n",
      "- Uses business-friendly language while maintaining technical accuracy\n",
      "- Clear section headings that guide executive decision-making\n",
      "- Avoids jargon while explaining complex concepts accessibly\n",
      "- Logical flow from problem to solution to business impact\n",
      "- Appropriate length and detail level for executive consumption\n",
      "\n",
      "### Relevance (4/5)\n",
      "Highly relevant to business executive needs:\n",
      "- Focuses on competitive advantages and cost implications\n",
      "- Emphasizes practical applications and market opportunities\n",
      "- Addresses ROI considerations (efficiency gains, resource reduction)\n",
      "- Discusses strategic implications for AI applications\n",
      "- Includes risk assessment through limitations discussion\n",
      "\n",
      "The summary successfully translates a highly technical research paper into actionable business intelligence, making it valuable for strategic decision-making around AI investments and technology adoption.\n",
      "\n",
      "**Overall: This is a strong executive summary that effectively bridges the gap between cutting-edge research and business strategy.**\n",
      "\n",
      "--- GENERAL AUDIENCE EVALUATION ---\n",
      "**Rating: 4 - Very Good**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4/5)\n",
      "The summary accurately represents the core concepts and findings from the Transformer paper. It correctly identifies:\n",
      "- The main innovation of replacing RNNs/CNNs with self-attention\n",
      "- Key technical components (multi-head attention, scaled dot-product attention, positional encoding)\n",
      "- Performance results (2.0+ BLEU improvement, state-of-the-art results)\n",
      "- The paper's historical significance\n",
      "\n",
      "Minor accuracy issue: The summary slightly oversimplifies some technical details, but this is appropriate for the general audience.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all major aspects of the paper:\n",
      "- Problem statement and motivation\n",
      "- Technical solution and architecture\n",
      "- Experimental results and performance\n",
      "- Broader implications and impact\n",
      "- Limitations\n",
      "\n",
      "It appropriately balances technical depth with accessibility, though it could have mentioned the specific training details (WMT datasets, computational requirements) more explicitly.\n",
      "\n",
      "### Clarity (5/5)\n",
      "Excellent clarity for a general audience:\n",
      "- Uses effective analogies (the cat sentence example for self-attention)\n",
      "- Explains technical concepts in accessible language\n",
      "- Well-structured with clear headings and logical flow\n",
      "- Avoids jargon while maintaining technical accuracy\n",
      "- The progression from problem → solution → results → impact is very clear\n",
      "\n",
      "### Relevance (4/5)\n",
      "Highly relevant to the general audience persona:\n",
      "- Focuses on the broader significance and real-world impact\n",
      "- Connects to familiar modern AI applications (BERT, GPT)\n",
      "- Explains why this matters beyond just technical achievements\n",
      "- Appropriately emphasizes the transformative nature of the work\n",
      "\n",
      "The summary successfully transforms a highly technical paper into an engaging and understandable narrative for non-experts while maintaining scientific accuracy. The use of analogies and real-world connections makes complex concepts accessible without oversimplifying to the point of inaccuracy.\n"
     ]
    }
   ],
   "source": [
    "# Local PDF file\n",
    "source = \"Attention is All You Need.pdf\"\n",
    "\n",
    "# ArXiv paper by ID\n",
    "# source = \"arxiv:1706.03762\" # ArXiv ID for Attention is All You Need Paper\n",
    "\n",
    "# ArXiv URL\n",
    "# source = \"https://arxiv.org/abs/1706.03762\" # URL for Attention is All You Need Paper\n",
    "\n",
    "# Custom query for focused summarization\n",
    "query = \"Summarize this research paper\"\n",
    "\n",
    "# Specify the LLM model\n",
    "llm_model = llm_model\n",
    "\n",
    "print(f\"Attempting to summarize {source} with model {llm_model} and query '{query}'\")\n",
    "\n",
    "try:\n",
    "    # Run summarization\n",
    "    results = get_paper_summary(source, query)\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        print(f\"\\n--- ERROR ---\")\n",
    "        print(f\"An error occurred during summarization: {results['error']}\")\n",
    "    else:\n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PAPER SUMMARIZATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nSource: {results['source']}\")\n",
    "        print(f\"Query: {results['query']}\")\n",
    "        print(f\"LLM Model Used: {llm_model}\")\n",
    "        print(f\"Processing: {results['processing_info']}\")\n",
    "        print(f\"Context Length: {results['context_length']:,} characters\")\n",
    "        \n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(\"PERSONA SUMMARIES\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for persona in results['personas']:\n",
    "            print(f\"\\n--- {persona.upper()} ---\")\n",
    "            print(results['summaries'][persona])\n",
    "        \n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(\"SUMMARY EVALUATIONS\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for persona in results['personas']:\n",
    "            print(f\"\\n--- {persona.upper()} EVALUATION ---\")\n",
    "            print(results['evaluations'][persona])\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n--- CRITICAL ERROR ---\")\n",
    "    print(f\"An unexpected critical error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
