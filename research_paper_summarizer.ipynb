{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a412f0",
   "metadata": {},
   "source": [
    "# Research Paper Summarizer for Different Personas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4625a138",
   "metadata": {},
   "source": [
    "## 1.Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826176dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Necessary Libraries\n",
    "import os\n",
    "import re\n",
    "import anthropic\n",
    "import torch\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# For Document Loading\n",
    "from langchain.document_loaders import UnstructuredFileLoader, WebBaseLoader, ArxivLoader, PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "# For Text Splitting and Processing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import nltk\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# For Embedding and Vector Store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# For LLM and Prompts\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# For setting up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c4178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read .env file to get Anthropic API key\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv('env')) # read local .env file\n",
    "anthropic_api_key=os.environ['ANTHROPIC_API_KEY']\n",
    "\n",
    "# Initialize Anthropic client\n",
    "anthropic_client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaa99e4",
   "metadata": {},
   "source": [
    "## 2. Define & Initialize LLM Models & Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26cfd4",
   "metadata": {},
   "source": [
    "### Define LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53095ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Model for Summarization\n",
    "llm_model = \"claude-3-7-sonnet-20250219\"\n",
    "default_llm = ChatAnthropic(model=llm_model, temperature=0.0, api_key=anthropic_api_key)\n",
    "\n",
    "# LLM Model for Evaluation\n",
    "llm_eval_model = \"claude-sonnet-4-20250514\"\n",
    "default_llm_eval = ChatAnthropic(model=llm_eval_model, temperature=0.5, api_key=anthropic_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d8189",
   "metadata": {},
   "source": [
    "### Initialize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a4505a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace Embeddings\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8e945",
   "metadata": {},
   "source": [
    "## 3. Create Important Classes & Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f164dcb",
   "metadata": {},
   "source": [
    "### 1. Create SentenceWindow and SentenceWindowRetriever Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c78166c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceWindow and SentenceWindowRetriever classes defined successfully.\n"
     ]
    }
   ],
   "source": [
    "class SentenceWindow:\n",
    "    \"\"\" A sentence with its surrounding context window. \"\"\"\n",
    "    def __init__(self, sentence, window_context, sentence_index, metadata):\n",
    "        self.sentence = sentence\n",
    "        self.window_context = window_context\n",
    "        self.sentence_index = sentence_index\n",
    "        self.metadata = metadata\n",
    "\n",
    "class SentenceWindowRetriever:\n",
    "    \"\"\" To retrieve sentences with their context windows. \"\"\"\n",
    "    def __init__(self, window_size: int =2):\n",
    "        self.window_size = window_size\n",
    "        self.sentence_windows = []\n",
    "        self.vector_store = None\n",
    "\n",
    "    def create_sentence_windows(self, documents):\n",
    "        \"\"\" Create sentence windows from the documents\"\"\"\n",
    "        logger.info(\"Creating sentence windows....\")\n",
    "        all_windows = []\n",
    "\n",
    "        for doc in documents:\n",
    "            # Split into sentences using NLTK\n",
    "            sentences = nltk.sent_tokenize(doc.page_content)\n",
    "\n",
    "            # Creare windows around each sentence\n",
    "            for i, sentence in enumerate(sentences):\n",
    "                # Define window boundaries\n",
    "                start_index = max(0, i - self.window_size)\n",
    "                end_index = min(len(sentences), i + self.window_size + 1)\n",
    "\n",
    "                # Create context window\n",
    "                window_sentences = sentences[start_index:end_index]\n",
    "                window_context = \" \".join(window_sentences)\n",
    "\n",
    "                # Create metadata\n",
    "                metadata = {\n",
    "                    **doc.metadata,\n",
    "                    \"sentence_index\":i,\n",
    "                    \"total_sentences\": len(sentences),\n",
    "                    \"window_start\": start_index,\n",
    "                    \"window_end\": end_index\n",
    "                }\n",
    "\n",
    "                window = SentenceWindow(\n",
    "                    sentence=sentence,\n",
    "                    window_context=window_context,\n",
    "                    sentence_index=i,\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                all_windows.append(window)\n",
    "\n",
    "        logger.info(f\"Created {len(all_windows)} sentence windows.\")\n",
    "        self.sentence_windows = all_windows\n",
    "        return all_windows\n",
    "    \n",
    "    def build_vectorstore(self, embeddings):\n",
    "        \"\"\" Build a vector store from the sentence windows. \"\"\"\n",
    "        if not self.sentence_windows:\n",
    "            raise ValueError(\"No sentence windows created. Call create_sentence_windows() first.\")\n",
    "        \n",
    "        logger.info(\"Building vector store from sentence windows...\")\n",
    "\n",
    "        # Convert windows to documents for vector store\n",
    "        window_docs = []\n",
    "        for window in self.sentence_windows:\n",
    "            doc = Document(\n",
    "                page_content=window.window_context,\n",
    "                metadata={\n",
    "                    **window.metadata,\n",
    "                    \"core_sentence\": window.sentence\n",
    "                }\n",
    "            )\n",
    "            window_docs.append(doc)\n",
    "\n",
    "        # Build FAISS vector store (more efficient than in-memory)\n",
    "        self.vectorstore = FAISS.from_documents(window_docs, embeddings)\n",
    "        logger.info(\"Vector store built successfully.\")\n",
    "        return self.vectorstore\n",
    "    \n",
    "    def retrieve(self, query, k=5):\n",
    "        \"\"\" Retrieve relevant sentence windows \"\"\"\n",
    "        if not self.vectorstore:\n",
    "            raise ValueError(\"Vector store not built. Call build_vectorstore() first.\")\n",
    "        \n",
    "        return self.vectorstore.similarity_search(query, k=k)\n",
    "    \n",
    "print(\"SentenceWindow and SentenceWindowRetriever classes defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd20be74",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77339a17",
   "metadata": {},
   "source": [
    "### 2. Create DocumentProcessor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e43ccf10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DocumentProcessor class defined.\n"
     ]
    }
   ],
   "source": [
    "class DocumentProcessor:\n",
    "    \"\"\" Handles document loading and preprocessing\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def load_documents(source):\n",
    "        \"\"\" Load document from  various sources with better error handling\"\"\"\n",
    "        logger.info(f\"Loading document from {source}\")\n",
    "\n",
    "        try:\n",
    "            # Determine the source type(whether it's a PDF file or a ARXIV link) and use appropriate loader\n",
    "            if source.startswith(\"http\") and \"arxiv.org\" in source:\n",
    "                # Extract arXiv ID from URL\n",
    "                arxiv_id = DocumentProcessor._extract_arxiv_id(source)\n",
    "                if arxiv_id:\n",
    "                    loader = ArxivLoader(query=arxiv_id, load_max_docs=1)\n",
    "                else:\n",
    "                    loader = WebBaseLoader(source)\n",
    "            elif source.startswith(\"http\"):\n",
    "                loader = WebBaseLoader(source)\n",
    "            elif source.startswith(\"arxiv:\"):\n",
    "                # Handle direct arXiv IDs \n",
    "                arxiv_id = source.replace(\"arxiv:\", \"\")\n",
    "                loader = ArxivLoader(query=arxiv_id, load_max_docs=1)\n",
    "            elif os.path.isfile(source):\n",
    "                file_extension = os.path.splitext(source)[1].lower()\n",
    "                if file_extension == \".pdf\":\n",
    "                    # Use PyPDFLoader for PDF files \n",
    "                    logging.info(f\"Detected PDF file, using PyPDFLoader for {source}\")\n",
    "                    loader = PyPDFLoader(source)\n",
    "            else:\n",
    "                # Raise an error if the source type is not recognized or file doesn't exist\n",
    "                raise ValueError(f\"Unsupported document source or file not found: {source}\")\n",
    "\n",
    "            documents = loader.load()\n",
    "\n",
    "            if not documents:\n",
    "                raise ValueError(\"No documents loaded from the specified source.\")\n",
    "            \n",
    "            logger.info(f\"Successfully loaded {len(documents)} document(s) from {source}\")\n",
    "            return documents\n",
    "        \n",
    "        except FileNotFoundError as e:\n",
    "            logger.error(f\"File not found error while loading document: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"An unexpected error occurred during document loading: {e}\")\n",
    "            raise\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_arxiv_id(url):\n",
    "        \"\"\" Extract arXiv ID from an arXiv URL\"\"\"\n",
    "        patterns = [\n",
    "            r'arxiv\\.org/abs/(\\d+\\.\\d+)',  # e.g., https://arxiv.org/abs/1234.5678\n",
    "            r'arxiv\\.org/pdf/(\\d+\\.\\d+)'   # e.g., https://arxiv.org/pdf/1234.5678\n",
    "        ]\n",
    "\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def preprocess_documents(documents):\n",
    "        \"\"\" Cleans and preprocesses a list of LangChain Document object\"\"\"\n",
    "\n",
    "        logger.info(\"Starting document preprocessing...\")\n",
    "\n",
    "        processed_docs = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            content = doc.page_content\n",
    "\n",
    "            # Remove excessive whitespace \n",
    "            content = re.sub(r'\\\\s+',' ', content).strip()\n",
    "\n",
    "            # Remove common artifacts like isolated page numbers \n",
    "            # Use regex to look for a newline, optional whitespace, digits, optional whitespace, and another newline\n",
    "            content = re.sub(r'\\\\n\\\\s*\\\\d+\\\\s*\\\\n', '\\\\n', content)\n",
    "\n",
    "        # Filter out documents with very short content after cleaning\n",
    "            if len(content) < 100:\n",
    "                logger.warning(f\"Skipping document {i} due to very short content after preprocessing (length: {len(content)}).\")\n",
    "                continue\n",
    "\n",
    "            # Create a new Document object with cleaned content\n",
    "            processed_doc = Document(page_content=content, metadata=doc.metadata)\n",
    "            processed_docs.append(processed_doc)\n",
    "\n",
    "        logger.info(f\"Finished preprocessing. Original documents: {len(documents)}, Preprocessed documents: {len(processed_docs)}\")\n",
    "        return processed_docs\n",
    "\n",
    "print(\"DocumentProcessor class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2930b",
   "metadata": {},
   "source": [
    "### 3. Create PersonaPrompts Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77ceddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PersonaPrompts class defined.\n"
     ]
    }
   ],
   "source": [
    "class PersonaPrompts:\n",
    "    \"\"\" Manages persona-specific prompts\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def get_personas():\n",
    "        \"\"\" Define different persona descriptions\"\"\"\n",
    "        return {\n",
    "            \"Data Scientist\": (\"a data scientist with strong knowledge in machine learning and deep learning who is currently learning generative AI. \"\n",
    "                \"Focus on practical applications, data requirements, model performance metrics, implementation considerations, \"\n",
    "                \"and how this relates to traditional ML/DL approaches you already know. Use technical language but explain generative AI concepts clearly.\"),\n",
    "            \"AI Engineer\": (\n",
    "                \"a senior AI engineer responsible for implementing and deploying ML systems. \"\n",
    "                \"Focus on practical implementation details, computational requirements, scalability considerations, \"\n",
    "                \"integration challenges, and performance metrics. Use engineering-focused language.\"\n",
    "            ),\n",
    "            \"Graduate Student\": (\n",
    "                \"a graduate student studying machine learning who needs to understand this paper for research. \"\n",
    "                \"Explain the core problem, methodology, key findings, and significance. \"\n",
    "                \"Use clear technical language but explain complex concepts.\"\n",
    "            ),\n",
    "            \"Business Executive\": (\n",
    "                \"a business executive with limited technical background who needs to understand the business impact. \"\n",
    "                \"Focus on the problem being solved, potential applications, market implications, \"\n",
    "                \"competitive advantages, and ROI considerations. Avoid technical jargon.\"\n",
    "            ),\n",
    "            \"General Audience\": (\n",
    "                \"explaining to an educated general audience with no AI background. \"\n",
    "                \"Use simple language, analogies, and focus on the big picture: what problem is solved, \"\n",
    "                \"how it works in simple terms, and why it matters.\"\n",
    "            )\n",
    "        }\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_prompt_templates():\n",
    "        \"\"\"Create persona-specific prompt templates.\"\"\"\n",
    "        personas = PersonaPrompts.get_personas()\n",
    "        templates = {}\n",
    "        \n",
    "        base_template = \"\"\"You are {persona_description}\n",
    "\n",
    "                            Based on the following research paper content, provide a comprehensive summary that addresses:\n",
    "                            1. The main problem or research question\n",
    "                            2. The proposed approach/solution\n",
    "                            3. Key findings and results \n",
    "                            4. Significance and implications\n",
    "                            5. Limitations or future work (if mentioned)\n",
    "\n",
    "                            Keep your summary focused on aspects most relevant to your perspective and audience.\n",
    "\n",
    "                            Research Paper Content:\n",
    "                            {context}\n",
    "\n",
    "                            Summary:\"\"\"\n",
    "        \n",
    "        for name, description in personas.items():\n",
    "            templates[name] =  ChatPromptTemplate.from_template(base_template).partial(persona_description=description)\n",
    "        \n",
    "        return templates\n",
    "\n",
    "print(\"PersonaPrompts class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25557ca5",
   "metadata": {},
   "source": [
    "### 4. Create SummaryEvaluator Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc927cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SummaryEvaluator class defined with model model='claude-sonnet-4-20250514' temperature=0.5 anthropic_api_url='https://api.anthropic.com' anthropic_api_key=SecretStr('**********') model_kwargs={}.\n"
     ]
    }
   ],
   "source": [
    "class SummaryEvaluator:\n",
    "    \"\"\"Evaluates summary quality using LLM-as-a-judge.\"\"\"\n",
    "\n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "        self.eval_prompt = self._create_evaluation_prompt()\n",
    "\n",
    "    def _create_evaluation_prompt(self):\n",
    "        \"\"\"Create evaluation prompt template.\"\"\"\n",
    "        template = \"\"\"You are an expert evaluator assessing the quality for research paper summaries.\n",
    "\n",
    "                        Evaluate the following summary based on these criteria:\n",
    "                        1. Accuracy: Does it correctly represent the source material?\n",
    "                        2. Completeness: Does it cover the key points appropriately?\n",
    "                        3. Clarity: Is it well-written and understandable for the target audience?\n",
    "                        4. Relevance: Does it focus on aspects relevant to the specified persona?\n",
    "\n",
    "                        Rate the summary on a scale of 1-5 where:\n",
    "                        1 = Poor (major inaccuracies, missing key points, unclear)\n",
    "                        2 = Fair (some issues with accuracy or completeness)\n",
    "                        3 = Good (mostly accurate and complete, minor issues)\n",
    "                        4 = Very Good (accurate, complete, well-written)\n",
    "                        5 = Excellent (outstanding in all criteria)\n",
    "\n",
    "                        Provide your rating and detailed justification.\n",
    "\n",
    "                        Source Material:\n",
    "                        {context}\n",
    "\n",
    "                        Summary to Evaluate:\n",
    "                        {summary}\n",
    "\n",
    "                        Persona: {persona}\n",
    "\n",
    "                        Evaluation:\"\"\"\n",
    "        return ChatPromptTemplate.from_template(template)\n",
    "    \n",
    "    def evaluate(self, summary, context, persona):\n",
    "        \"\"\" Evaluate a single summary.\"\"\"\n",
    "        try:\n",
    "            chain = self.eval_prompt | self.llm | StrOutputParser()\n",
    "            evaluation = chain.invoke({\n",
    "                \"summary\": summary,\n",
    "                \"context\": context,\n",
    "                \"persona\": persona\n",
    "            })\n",
    "            return evaluation\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error evaluating summary: {e}\")\n",
    "            return f\"Evaluation failed: {str(e)}\"\n",
    "\n",
    "print(f\"SummaryEvaluator class defined with model {default_llm_eval}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c008d3",
   "metadata": {},
   "source": [
    "### 5. Create PaperSummarizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93088a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PaperSummarizer class defined\n"
     ]
    }
   ],
   "source": [
    "class PaperSummarizer:\n",
    "    \"\"\" Main class responsible for the summarization process.\"\"\"\n",
    "\n",
    "    def __init__(self, llm, window_size=2):\n",
    "        self.llm = default_llm\n",
    "        self.embeddings = embeddings_model\n",
    "        self.retriever = SentenceWindowRetriever(window_size=window_size)\n",
    "        self.persona_prompts = PersonaPrompts.create_prompt_templates()\n",
    "        self.evaluator = SummaryEvaluator(default_llm_eval)\n",
    "\n",
    "        logger.info(f\"Initialized PaperSummarizer with model {llm_model}\")\n",
    "\n",
    "\n",
    "    def process_document(self, source):\n",
    "        \"\"\" Load and process document, return processed documents and context.\"\"\"\n",
    "        # Load document\n",
    "        documents = DocumentProcessor.load_documents(source)\n",
    "\n",
    "        # Preprocess\n",
    "        processed_docs = DocumentProcessor.preprocess_documents(documents)\n",
    "\n",
    "        if not processed_docs:\n",
    "            raise ValueError(\"No valid documents after preprocessing.\")\n",
    "        \n",
    "        # Create sentence windows\n",
    "        windows = self.retriever.create_sentence_windows(processed_docs)\n",
    "\n",
    "        # Build vectorstore\n",
    "        self.retriever.build_vectorstore(self.embeddings)\n",
    "\n",
    "        return processed_docs, f\"Processed {len(windows)} sentence windows\"\n",
    "    \n",
    "\n",
    "    def generate_summaries(self, query=\"Summarize this research paper\"):\n",
    "        \"\"\"Generate summaries for all personas.\"\"\"\n",
    "        # Retrieve relevant context\n",
    "        retrieved_docs = self.retriever.retrieve(query, k=150) # Get more context\n",
    "        context = \"/n/n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "        if not context.strip():\n",
    "            raise ValueError(\"No relevant context retrieved\")\n",
    "        \n",
    "        logger.info(f\"Retrieved context length: {len(context)} characters\")\n",
    "\n",
    "        # Generate summaries for each persona\n",
    "        summaries = {}\n",
    "        for persona, template in self.persona_prompts.items():\n",
    "            try:\n",
    "                logger.info(f\"Generating summary for: {persona}\")\n",
    "\n",
    "                chain = template | self.llm | StrOutputParser()\n",
    "                summary = chain.invoke({\"context\": context})\n",
    "                summaries[persona] = summary\n",
    "\n",
    "                # Add a small delay to avoid hitting API rate limits\n",
    "                time.sleep(30)\n",
    "\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error generating summary for {persona}: {e}\")\n",
    "                summaries[persona] = f\"Error generating summary: {str(e)}\"\n",
    "        \n",
    "        return summaries, context\n",
    "    \n",
    "\n",
    "    def evaluate_summaries(self, summaries, context):\n",
    "        \"\"\" Evaluate all generated summaries.\"\"\"\n",
    "        evaluations = {}\n",
    "\n",
    "        for persona_name, summary in summaries.items():\n",
    "            logger.info(f\"Evaluating summary for {persona_name}\")\n",
    "            evaluation = self.evaluator.evaluate(summary, context, persona_name)\n",
    "            evaluations[persona_name] = evaluation\n",
    "            \n",
    "            # Add a small delay to avoid hitting API rate limits\n",
    "            time.sleep(30)\n",
    "        \n",
    "        return evaluations\n",
    "    \n",
    "\n",
    "    def summarize_paper(self, source, query = \"Summarize this research paper\"):\n",
    "        \"\"\"Complete process: load, preprocess, summarize, and evaluate.\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Starting paper summarizarion for {source}\")\n",
    "\n",
    "            # Process document\n",
    "            processed_docs, processing_info = self.process_document(source)\n",
    "\n",
    "            # Generate summaries\n",
    "            summaries, context = self.generate_summaries(query)\n",
    "\n",
    "            # Evaluate summaries\n",
    "            evaluations = self.evaluate_summaries(summaries, context)\n",
    "\n",
    "            # Compile results\n",
    "            results = {\n",
    "                \"source\": source,\n",
    "                \"query\": query,\n",
    "                \"processing_info\": processing_info,\n",
    "                \"context_length\": len(context),\n",
    "                \"summaries\": summaries,\n",
    "                \"evaluations\": evaluations,\n",
    "                \"personas\": list(summaries.keys())\n",
    "            }\n",
    "\n",
    "            logger.info(\"Paper summarization completed successfully.\")\n",
    "            return results\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in summarization process: {e}\")\n",
    "            raise\n",
    "\n",
    "print(\"PaperSummarizer class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4143fa8e",
   "metadata": {},
   "source": [
    "### 6. Create get_paper_summary Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8531b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_paper_summary function defined.\n"
     ]
    }
   ],
   "source": [
    "def get_paper_summary(source:str, query:str = \"Summarize this research paper\") -> dict:\n",
    "    \"\"\" Summarizes a research paper from a given source.\"\"\"\n",
    "    try:\n",
    "        summarizer = PaperSummarizer(default_llm)\n",
    "        results = summarizer.summarize_paper(source, query)\n",
    "        return results\n",
    "    except ValueError as ve:\n",
    "        logger.error(f\"Configuration error: {ve}\")\n",
    "        return {\"error\": str(ve)}\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An unexpected error occurred: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "print(\"get_paper_summary function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e481a5fa",
   "metadata": {},
   "source": [
    "## 3. Generating Summaries for Personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c77b4682",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized PaperSummarizer with model claude-3-7-sonnet-20250219\n",
      "INFO:__main__:Starting paper summarizarion for Attention is All You Need.pdf\n",
      "INFO:__main__:Loading document from Attention is All You Need.pdf\n",
      "INFO:root:Detected PDF file, using PyPDFLoader for Attention is All You Need.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to summarize Attention is All You Need.pdf with model claude-3-7-sonnet-20250219 and query 'Summarize this research paper'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Successfully loaded 15 document(s) from Attention is All You Need.pdf\n",
      "INFO:__main__:Starting document preprocessing...\n",
      "INFO:__main__:Finished preprocessing. Original documents: 15, Preprocessed documents: 15\n",
      "INFO:__main__:Creating sentence windows....\n",
      "INFO:__main__:Created 373 sentence windows.\n",
      "INFO:__main__:Building vector store from sentence windows...\n",
      "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "INFO:__main__:Vector store built successfully.\n",
      "INFO:__main__:Retrieved context length: 61788 characters\n",
      "INFO:__main__:Generating summary for: Data Scientist\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: AI Engineer\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: Graduate Student\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: Business Executive\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generating summary for: General Audience\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for Data Scientist\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.451464 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.984744 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:__main__:Error evaluating summary: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:__main__:Evaluating summary for AI Engineer\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.379079 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.933286 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:__main__:Error evaluating summary: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:__main__:Evaluating summary for Graduate Student\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.492899 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "INFO:anthropic._base_client:Retrying request to /v1/messages in 0.954182 seconds\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 529 \"\n",
      "ERROR:__main__:Error evaluating summary: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "INFO:__main__:Evaluating summary for Business Executive\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Evaluating summary for General Audience\n",
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Paper summarization completed successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PAPER SUMMARIZATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "Source: Attention is All You Need.pdf\n",
      "Query: Summarize this research paper\n",
      "LLM Model Used: claude-3-7-sonnet-20250219\n",
      "Processing: Processed 373 sentence windows\n",
      "Context Length: 61,788 characters\n",
      "\n",
      "------------------------------------------------------------\n",
      "PERSONA SUMMARIES\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- DATA SCIENTIST ---\n",
      "# Comprehensive Summary of \"Attention Is All You Need\"\n",
      "\n",
      "## 1. Main Problem/Research Question\n",
      "The paper addresses limitations in sequence transduction models that rely on complex recurrent or convolutional neural networks with encoder-decoder architectures. Traditional models face challenges in learning long-distance dependencies and have limited parallelization capabilities during training. The authors investigate whether attention mechanisms alone can create effective sequence transduction models without recurrence or convolution.\n",
      "\n",
      "## 2. Proposed Approach/Solution\n",
      "The authors introduce the Transformer, a novel neural network architecture based entirely on attention mechanisms:\n",
      "\n",
      "- **Architecture**: Encoder-decoder structure with stacked self-attention and point-wise fully connected layers\n",
      "- **Key Components**:\n",
      "  - **Multi-Head Attention**: Projects queries, keys, and values h times with different learned projections, allowing the model to jointly attend to information from different representation subspaces\n",
      "  - **Scaled Dot-Product Attention**: Computes attention by taking dot products of queries with keys, scaling by 1/√dk, and applying softmax to obtain weights for values\n",
      "  - **Position-wise Feed-Forward Networks**: Applied to each position separately and identically\n",
      "  - **Positional Encoding**: Adds information about token position in the sequence without using recurrence\n",
      "  - **Residual Connections and Layer Normalization**: Used around each sub-layer to facilitate training\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "- The Transformer outperforms previous state-of-the-art models on machine translation tasks:\n",
      "  - Achieved 28.4 BLEU score on WMT 2014 English-to-German translation, exceeding previous best models by more than 2.0 BLEU points\n",
      "  - Achieved 41.0 BLEU score on WMT 2014 English-to-French translation\n",
      "- The model requires significantly less training time compared to RNN/CNN-based architectures\n",
      "- Ablation studies showed:\n",
      "  - Multi-head attention is crucial (single-head attention performed 0.9 BLEU worse)\n",
      "  - Attention key size (dk) significantly impacts model quality\n",
      "  - Model size and dropout are important for performance\n",
      "\n",
      "## 4. Significance and Implications\n",
      "- Demonstrates that self-attention can replace recurrence and convolution entirely in sequence transduction tasks\n",
      "- Enables significantly more parallelization during training, reducing training time\n",
      "- Provides constant-time path length between any two positions in the sequence, addressing the challenge of learning long-range dependencies\n",
      "- Creates more interpretable models, with attention heads that learn to perform different tasks related to syntactic and semantic structure\n",
      "- Establishes a new architecture paradigm that has since become foundational for many NLP advances\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "- The model has reduced effective resolution due to averaging attention-weighted positions, which is partially addressed by multi-head attention\n",
      "- The authors suggest that more sophisticated compatibility functions than dot product may be beneficial for attention mechanisms\n",
      "- The paper notes that self-attention could yield more interpretable models, but further work is needed to fully explore this potential\n",
      "- While the paper focuses on machine translation, the architecture's application to other sequence tasks represents an area for future exploration\n",
      "\n",
      "From a data science perspective, the Transformer architecture represents a significant advancement in how we can design models for sequential data, offering better parallelization, improved handling of long-range dependencies, and potentially more interpretable representations compared to traditional RNN/CNN approaches.\n",
      "\n",
      "--- AI ENGINEER ---\n",
      "# Engineering Analysis: \"Attention Is All You Need\" Paper\n",
      "\n",
      "## 1. Main Problem/Research Question\n",
      "The paper addresses limitations in sequence transduction models that rely on complex recurrent or convolutional neural networks with encoder-decoder architectures. Traditional approaches face challenges with long-range dependencies and limited parallelization capabilities, creating computational bottlenecks in training and inference pipelines.\n",
      "\n",
      "## 2. Proposed Approach/Solution\n",
      "The authors introduce the Transformer architecture, which completely eliminates recurrence and convolutions in favor of self-attention mechanisms. Key technical components include:\n",
      "\n",
      "- **Multi-head attention**: Projects queries, keys, and values h times with different learned projections, allowing the model to jointly attend to information from different representation subspaces\n",
      "- **Scaled dot-product attention**: Computes attention weights using dot products scaled by 1/√dk to prevent gradient vanishing in deeper layers\n",
      "- **Position-wise feed-forward networks**: Applied to each position separately and identically\n",
      "- **Residual connections and layer normalization**: Used around each sub-layer to facilitate gradient flow\n",
      "- **Positional encodings**: Added to input embeddings to retain sequence order information\n",
      "\n",
      "The architecture consists of encoder and decoder stacks (6 layers each in the base model), with self-attention mechanisms allowing constant-time computation between any positions in the sequence.\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "- The Transformer architecture achieved state-of-the-art results on machine translation benchmarks:\n",
      "  - 28.4 BLEU on WMT 2014 English-to-German (2.0+ BLEU improvement over previous SOTA)\n",
      "  - 41.0 BLEU on WMT 2014 English-to-French\n",
      "- Training efficiency was significantly improved: the model required only a fraction of the computational resources compared to previous approaches\n",
      "- Ablation studies showed that multi-head attention is crucial (8 heads optimal in their experiments)\n",
      "- Attention key size (dk) significantly impacts model quality\n",
      "- Visualization of attention heads revealed interpretable patterns, with different heads specializing in different linguistic phenomena (e.g., anaphora resolution, syntactic dependencies)\n",
      "\n",
      "## 4. Significance and Implications\n",
      "From an engineering perspective, the Transformer architecture represents a paradigm shift in sequence modeling with several practical advantages:\n",
      "\n",
      "- **Parallelization**: By removing sequential computation dependencies, the model enables much higher throughput during training on modern hardware (GPUs/TPUs)\n",
      "- **Computational efficiency**: Constant-time path length between any two positions in the sequence\n",
      "- **Scalability**: The architecture scales effectively with more compute resources\n",
      "- **Interpretability**: Self-attention mechanisms provide insights into model decision-making\n",
      "- **Implementation simplicity**: The architecture is conceptually simpler than complex RNN/CNN alternatives\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "- The model's effective resolution is reduced due to averaging attention-weighted positions\n",
      "- More sophisticated compatibility functions than dot product may be beneficial for attention mechanisms\n",
      "- The constant path length comes at the cost of reduced effective resolution\n",
      "- The model requires careful hyperparameter tuning, particularly for attention heads, key/value dimensions, and dropout rates\n",
      "- While not explicitly stated as future work, the visualizations suggest potential for further research into the interpretability of attention patterns and their relationship to linguistic structures\n",
      "\n",
      "This architecture has become the foundation for numerous state-of-the-art models in NLP and beyond, demonstrating exceptional engineering value through its scalability, parallelizability, and performance characteristics.\n",
      "\n",
      "--- GRADUATE STUDENT ---\n",
      "# Attention Is All You Need: A Comprehensive Summary\n",
      "\n",
      "## 1. Main Problem and Research Question\n",
      "\n",
      "The paper \"Attention Is All You Need\" addresses a fundamental challenge in sequence transduction tasks like machine translation: the reliance on complex recurrent or convolutional neural networks with encoder-decoder architectures. Traditional approaches faced limitations in modeling long-range dependencies and were difficult to parallelize. The authors investigate whether attention mechanisms alone could replace recurrent and convolutional components entirely, potentially improving both performance and computational efficiency.\n",
      "\n",
      "## 2. Proposed Approach/Solution\n",
      "\n",
      "The authors introduce the **Transformer**, a novel neural network architecture that relies solely on attention mechanisms without using recurrence or convolution. Key components include:\n",
      "\n",
      "- **Self-attention**: Allows the model to relate different positions in a sequence directly, regardless of distance\n",
      "- **Multi-head attention**: Projects queries, keys, and values into different representation subspaces, enabling the model to attend to information from different positions simultaneously\n",
      "- **Scaled dot-product attention**: A computationally efficient attention mechanism that scales dot products by 1/√d_k to prevent extremely small gradients\n",
      "- **Position-wise feed-forward networks**: Applied to each position separately and identically\n",
      "- **Positional encodings**: Added to input embeddings to provide information about token position\n",
      "\n",
      "The Transformer follows an encoder-decoder structure:\n",
      "- The encoder consists of 6 identical layers, each with multi-head self-attention and position-wise feed-forward networks\n",
      "- The decoder has a similar structure but includes an additional attention layer that attends to the encoder's output\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "\n",
      "The Transformer achieved state-of-the-art results on machine translation tasks:\n",
      "- On the WMT 2014 English-to-German translation task, the large Transformer model achieved a BLEU score of 28.4, outperforming previous models by more than 2.0 BLEU points\n",
      "- On the WMT 2014 English-to-French translation task, it achieved a BLEU score of 41.0\n",
      "- Even the base Transformer model surpassed all previously published models at a fraction of the training cost\n",
      "\n",
      "The authors' ablation studies revealed:\n",
      "- Multi-head attention outperforms single-head attention\n",
      "- Reducing the attention key size hurts model quality\n",
      "- Larger models perform better, and dropout is crucial for avoiding overfitting\n",
      "\n",
      "## 4. Significance and Implications\n",
      "\n",
      "The Transformer architecture represents a paradigm shift in sequence modeling by demonstrating that:\n",
      "- Self-attention can effectively replace recurrence and convolution in neural sequence models\n",
      "- The architecture allows for significantly more parallelization, reducing training time\n",
      "- The constant number of operations required to relate signals between positions (regardless of distance) helps capture long-range dependencies\n",
      "- The attention mechanisms produce interpretable models, with different attention heads learning distinct and meaningful patterns related to syntactic and semantic structure\n",
      "\n",
      "This work laid the foundation for subsequent transformer-based models that have revolutionized NLP, including BERT, GPT, and other large language models.\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "\n",
      "While not explicitly stated as limitations, the paper notes:\n",
      "- The attention mechanism's effective resolution is reduced due to averaging attention-weighted positions (mitigated by multi-head attention)\n",
      "- The dot-product attention mechanism may not be the most sophisticated compatibility function\n",
      "- The model requires careful tuning of hyperparameters like the number of attention heads\n",
      "\n",
      "The authors also suggest that self-attention could yield more interpretable models, presenting visualizations showing how different attention heads learn to perform different tasks related to syntactic and semantic structure.\n",
      "\n",
      "The Transformer architecture's success in machine translation suggested its potential applicability to other sequence modeling tasks, which has been extensively validated by subsequent research.\n",
      "\n",
      "--- BUSINESS EXECUTIVE ---\n",
      "# Business Executive Summary: \"Attention Is All You Need\"\n",
      "\n",
      "## 1. The Main Problem\n",
      "Traditional machine translation and language processing systems rely on complex recurrent or convolutional neural networks that are difficult to train, computationally expensive, and struggle with long-distance relationships in text. These limitations create bottlenecks in developing more efficient and accurate language processing applications.\n",
      "\n",
      "## 2. The Proposed Solution\n",
      "The researchers introduced a revolutionary new architecture called the \"Transformer\" that completely eliminates the need for recurrent networks or convolutions. Instead, it relies entirely on a mechanism called \"self-attention\" that allows the model to focus on relevant parts of input text regardless of their distance from each other. This approach makes the model both more powerful and more efficient.\n",
      "\n",
      "## 3. Key Findings and Results\n",
      "- The Transformer outperformed all previous translation systems by significant margins (over 2.0 BLEU points) on standard English-to-German translation tasks\n",
      "- It achieved state-of-the-art results on English-to-French translation with only 25% of the training costs of previous best models\n",
      "- The model demonstrated remarkable ability to handle long-distance dependencies in text\n",
      "- Different \"attention heads\" in the model learned to perform distinct linguistic tasks automatically\n",
      "\n",
      "## 4. Business Significance and Implications\n",
      "- **Efficiency**: The Transformer architecture is highly parallelizable, meaning it can be trained much faster than previous approaches, reducing development time and costs\n",
      "- **Scalability**: The model performs better as it gets larger, suggesting a clear path to continued improvement\n",
      "- **Versatility**: While demonstrated for translation, the approach can be applied to many language tasks including summarization, question answering, and content generation\n",
      "- **Interpretability**: The attention mechanism provides insights into how the model makes decisions, potentially addressing the \"black box\" problem of AI systems\n",
      "- **Cost reduction**: Faster training and inference means lower computing costs for language AI applications\n",
      "\n",
      "## 5. Limitations and Future Work\n",
      "The researchers note that while the Transformer excels at capturing relationships between words regardless of distance, there may be some loss of resolution due to the attention-averaging effect. They counteract this with their multi-head attention approach but suggest that more sophisticated compatibility functions than the dot product they used might yield further improvements.\n",
      "\n",
      "This breakthrough represents a fundamental shift in how language processing systems can be built, with significant implications for any business relying on language AI for customer service, content creation, translation, or information extraction.\n",
      "\n",
      "--- GENERAL AUDIENCE ---\n",
      "# Attention Is All You Need: Transforming Language Processing\n",
      "\n",
      "## The Main Problem\n",
      "\n",
      "Traditional machine translation and language processing systems relied on complex recurrent neural networks (RNNs) or convolutional neural networks (CNNs) that processed text sequentially. This sequential nature created a bottleneck: it was difficult for these models to capture relationships between words that were far apart in a sentence, and they couldn't be easily parallelized for faster computation.\n",
      "\n",
      "## The Proposed Solution: The Transformer\n",
      "\n",
      "The researchers introduced a revolutionary new architecture called the \"Transformer\" that completely eliminates recurrence and convolutions. Instead, it relies entirely on a mechanism called \"self-attention\" to process language.\n",
      "\n",
      "Think of self-attention like this: when you read a sentence, you naturally pay attention to different words to understand meaning. For example, in \"The cat, which was orange, ran across the street,\" you connect \"cat\" with \"ran\" despite the words between them. The Transformer mimics this ability by allowing each word to \"look at\" and connect with every other word in the sentence simultaneously.\n",
      "\n",
      "The key innovations include:\n",
      "\n",
      "1. **Multi-head attention**: Rather than having a single attention mechanism, the Transformer uses multiple \"attention heads\" that each focus on different aspects of the relationships between words.\n",
      "\n",
      "2. **Scaled dot-product attention**: A mathematical technique that helps the model efficiently determine which words should pay attention to each other.\n",
      "\n",
      "3. **Positional encoding**: Since the model processes all words at once (not sequentially), it needs a way to understand word order, which positional encoding provides.\n",
      "\n",
      "## Key Findings and Results\n",
      "\n",
      "The Transformer achieved breakthrough results:\n",
      "\n",
      "- On English-to-German translation, it outperformed previous state-of-the-art models by more than 2.0 BLEU points (a standard measure of translation quality).\n",
      "- On English-to-French translation, it achieved superior results while requiring only a quarter of the training resources of previous top models.\n",
      "- The model was significantly faster to train than RNN-based approaches because it could process all words in parallel rather than sequentially.\n",
      "\n",
      "The researchers also discovered that different attention heads learned to perform different linguistic tasks automatically. Some heads tracked grammatical relationships, while others followed semantic connections or resolved pronouns (like connecting \"its\" back to what it refers to).\n",
      "\n",
      "## Significance and Implications\n",
      "\n",
      "The Transformer architecture represented a paradigm shift in how we process language with computers:\n",
      "\n",
      "1. **Efficiency**: By enabling parallel processing, it made training much faster and more scalable.\n",
      "\n",
      "2. **Better performance**: It captured long-distance relationships between words more effectively than previous approaches.\n",
      "\n",
      "3. **Interpretability**: The attention patterns revealed how the model was making decisions, making it somewhat more transparent than previous \"black box\" approaches.\n",
      "\n",
      "This architecture became the foundation for virtually all modern language AI systems, including BERT, GPT, and other large language models that have transformed everything from search engines to writing assistants.\n",
      "\n",
      "## Limitations and Future Work\n",
      "\n",
      "The researchers noted some limitations, including:\n",
      "\n",
      "- The model's effectiveness decreased with too many attention heads\n",
      "- The compatibility function (how words relate to each other) might benefit from more sophisticated approaches than the dot product method they used\n",
      "- The attention mechanism trades off some resolution due to averaging, though multi-head attention helps counteract this\n",
      "\n",
      "The Transformer opened up numerous new research directions in natural language processing that continue to be explored today.\n",
      "\n",
      "------------------------------------------------------------\n",
      "SUMMARY EVALUATIONS\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- DATA SCIENTIST EVALUATION ---\n",
      "Evaluation failed: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "\n",
      "--- AI ENGINEER EVALUATION ---\n",
      "Evaluation failed: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "\n",
      "--- GRADUATE STUDENT EVALUATION ---\n",
      "Evaluation failed: Error code: 529 - {'type': 'error', 'error': {'type': 'overloaded_error', 'message': 'Overloaded'}}\n",
      "\n",
      "--- BUSINESS EXECUTIVE EVALUATION ---\n",
      "**Rating: 4 - Very Good**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4/5)\n",
      "The summary accurately represents the core concepts and findings from the Transformer paper. It correctly identifies:\n",
      "- The main innovation (self-attention replacing RNNs/CNNs)\n",
      "- Key performance metrics (2.0+ BLEU improvement, 25% training cost reduction)\n",
      "- The architecture's fundamental advantages (parallelization, handling long-distance dependencies)\n",
      "- The multi-head attention mechanism and its benefits\n",
      "\n",
      "Minor accuracy concern: The summary slightly oversimplifies some technical aspects, but this is appropriate for the business executive audience.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all essential points a business executive would need:\n",
      "- Problem statement and market context\n",
      "- Technical solution overview\n",
      "- Quantified performance results\n",
      "- Business implications and cost benefits\n",
      "- Scalability and versatility considerations\n",
      "- Limitations and future directions\n",
      "\n",
      "The structure follows a logical business presentation format that executives expect.\n",
      "\n",
      "### Clarity (5/5)\n",
      "Exceptionally well-written for the target audience:\n",
      "- Uses business-friendly language while maintaining technical accuracy\n",
      "- Avoids jargon and explains technical concepts in accessible terms\n",
      "- Clear section headings that guide executive decision-making\n",
      "- Quantified benefits that executives can easily grasp\n",
      "- Logical flow from problem to solution to business impact\n",
      "\n",
      "### Relevance (4/5)\n",
      "Highly relevant to business executives:\n",
      "- Focuses on cost reduction, efficiency gains, and competitive advantages\n",
      "- Addresses scalability and ROI considerations\n",
      "- Discusses practical applications across multiple business use cases\n",
      "- Mentions interpretability as a business risk mitigation factor\n",
      "- Provides clear implications for strategic planning\n",
      "\n",
      "The summary successfully translates complex technical research into actionable business intelligence, making it valuable for executive decision-making regarding AI investments and strategy.\n",
      "\n",
      "--- GENERAL AUDIENCE EVALUATION ---\n",
      "**Rating: 4 (Very Good)**\n",
      "\n",
      "## Detailed Justification:\n",
      "\n",
      "### Accuracy (4/5)\n",
      "The summary accurately represents the core concepts and findings from the Transformer paper. It correctly explains:\n",
      "- The fundamental shift from RNN/CNN architectures to attention-only models\n",
      "- Key technical innovations (multi-head attention, scaled dot-product attention, positional encoding)\n",
      "- Performance results (BLEU scores, training efficiency improvements)\n",
      "- The interpretability findings about different attention heads learning different tasks\n",
      "\n",
      "Minor accuracy issue: The summary could be more precise about some technical details, but this is appropriate given the general audience target.\n",
      "\n",
      "### Completeness (4/5)\n",
      "The summary covers all major aspects of the paper:\n",
      "- Problem motivation and context\n",
      "- Technical solution and architecture\n",
      "- Experimental results and performance metrics\n",
      "- Significance and broader impact\n",
      "- Limitations and future directions\n",
      "\n",
      "It appropriately balances technical depth with accessibility, though some implementation details are necessarily simplified for the general audience.\n",
      "\n",
      "### Clarity (5/5)\n",
      "Exceptionally well-written for a general audience. The summary:\n",
      "- Uses clear, accessible language while maintaining technical accuracy\n",
      "- Employs effective analogies (the cat sentence example for self-attention)\n",
      "- Follows a logical structure from problem to solution to results\n",
      "- Explains complex concepts without jargon\n",
      "- Uses formatting effectively to guide readers through the content\n",
      "\n",
      "### Relevance (4/5)\n",
      "Highly relevant to a general audience by:\n",
      "- Focusing on the broader significance and real-world applications\n",
      "- Connecting the research to familiar technologies (search engines, writing assistants)\n",
      "- Explaining why this research matters beyond academic circles\n",
      "- Providing context about how this work influenced modern AI systems\n",
      "\n",
      "The summary successfully translates highly technical research into content that would be valuable and understandable for educated non-specialists interested in understanding this foundational AI breakthrough.\n",
      "\n",
      "**Overall Assessment:** This is a very strong summary that effectively bridges the gap between complex technical research and general audience understanding while maintaining scientific accuracy.\n"
     ]
    }
   ],
   "source": [
    "# Local PDF file\n",
    "source = \"Attention is All You Need.pdf\"\n",
    "\n",
    "# ArXiv paper by ID\n",
    "# source = \"arxiv:1706.03762\" # ArXiv ID for Attention is All You Need Paper\n",
    "\n",
    "# ArXiv URL\n",
    "# source = \"https://arxiv.org/abs/1706.03762\" # URL for Attention is All You Need Paper\n",
    "\n",
    "# Custom query for focused summarization\n",
    "query = \"Summarize this research paper\"\n",
    "\n",
    "# Specify the LLM model\n",
    "llm_model = llm_model\n",
    "\n",
    "print(f\"Attempting to summarize {source} with model {llm_model} and query '{query}'\")\n",
    "\n",
    "try:\n",
    "    # Run summarization\n",
    "    results = get_paper_summary(source, query)\n",
    "    \n",
    "    if \"error\" in results:\n",
    "        print(f\"\\n--- ERROR ---\")\n",
    "        print(f\"An error occurred during summarization: {results['error']}\")\n",
    "    else:\n",
    "        # Print results\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"PAPER SUMMARIZATION RESULTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        print(f\"\\nSource: {results['source']}\")\n",
    "        print(f\"Query: {results['query']}\")\n",
    "        print(f\"LLM Model Used: {llm_model}\")\n",
    "        print(f\"Processing: {results['processing_info']}\")\n",
    "        print(f\"Context Length: {results['context_length']:,} characters\")\n",
    "        \n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(\"PERSONA SUMMARIES\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for persona in results['personas']:\n",
    "            print(f\"\\n--- {persona.upper()} ---\")\n",
    "            print(results['summaries'][persona])\n",
    "        \n",
    "        print(f\"\\n{'-'*60}\")\n",
    "        print(\"SUMMARY EVALUATIONS\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        for persona in results['personas']:\n",
    "            print(f\"\\n--- {persona.upper()} EVALUATION ---\")\n",
    "            print(results['evaluations'][persona])\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"\\n--- CRITICAL ERROR ---\")\n",
    "    print(f\"An unexpected critical error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
